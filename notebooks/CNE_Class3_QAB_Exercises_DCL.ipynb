{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO5ecsuMQtze9hEz2QSvI+3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mikislin/CNE25/blob/main/notebooks/CNE_Class3_QAB_Exercises_DCL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DeepLabCut on Single Mouse Data Demo\n",
        "\n",
        "üëã This notebook is a modified copy from [Github](https://colab.research.google.com/github/DeepLabCut/DeepLabCut/blob/master/examples/COLAB/COLAB_DEMO_mouse_openfield.ipynb), originally written by Mackenzie Mathis and contributors.\n",
        "\n",
        "‚ö†Ô∏è It has been edited for the 2025 CNE class!\n",
        "\n",
        "\n",
        "This notebook illustrates how to use DeepLabCut and Colab to:\n",
        "\n",
        "\n",
        "This notebook illustrates how to use the cloud to:\n",
        "\n",
        "- load demo data\n",
        "- create a training set\n",
        "- train a network\n",
        "- evaluate a network\n",
        "- analyze a novel video\n",
        "\n"
      ],
      "metadata": {
        "id": "3CD99r_Eeo-Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ew6r4hotoQjt"
      },
      "outputs": [],
      "source": [
        "# Clone the entire deeplabcut repo so we can use the demo data:\n",
        "!git clone -l -s https://github.com/DeepLabCut/DeepLabCut.git cloned-DLC-repo\n",
        "%cd cloned-DLC-repo\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the latest DeepLabCut version (this will take a few minutes to install all the dependencies!)\n",
        "%cd /content/cloned-DLC-repo/\n",
        "%pip install \".\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "TIjBPeATfH_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The installation error is expected after installing DLC on the colab\n",
        "PLEASE, click \"restart runtime\" from the output above before proceeding!"
      ],
      "metadata": {
        "id": "4osstnL-fme9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import deeplabcut"
      ],
      "metadata": {
        "id": "VmNidvpCfZDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rW1JlxtxbuwJ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import pandas as pd\n",
        "from ipywidgets import interact, IntSlider\n",
        "import os\n",
        "import yaml\n",
        "import numpy as np\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Preview the experimental data\n",
        "\n",
        "# Replace with the actual filename or full path\n",
        "video_path = '/content/cloned-DLC-repo/examples/openfield-Pranav-2018-10-30/videos/m3v1mp4.mp4'\n",
        "\n",
        "# Read the video file in binary mode\n",
        "with open(video_path, 'rb') as f:\n",
        "    mp4_bytes = f.read()\n",
        "\n",
        "# Encode the video bytes to base64\n",
        "mp4_base64 = b64encode(mp4_bytes).decode()\n",
        "\n",
        "# Create the HTML for embedding the video\n",
        "html_code = f\"\"\"\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "    <source src=\"data:video/mp4;base64,{mp4_base64}\" type=\"video/mp4\">\n",
        "    Your browser does not support the video tag.\n",
        "\n",
        "</video>\n",
        "\"\"\"\n",
        "\n",
        "# Display the HTML\n",
        "display(HTML(html_code))"
      ],
      "metadata": {
        "id": "AlFpO99bFzY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create a path variable that links to the config file:\n",
        "path_config_file = '/content/cloned-DLC-repo/examples/openfield-Pranav-2018-10-30/config.yaml'\n",
        "\n",
        "# Loading example data set:\n",
        "deeplabcut.load_demo_data(path_config_file)\n",
        "\n",
        "# Automatically update some hyperparameters for training,\n",
        "# here rotations to +/- 180 degrees. This can be helpful for optimizing performance.\n",
        "# see Primer -- Mathis et al. Neuron 2020\n",
        "from deeplabcut.core.config import read_config_as_dict\n",
        "import deeplabcut.pose_estimation_pytorch as dlc_torch\n",
        "\n",
        "loader = dlc_torch.DLCLoader(\n",
        "    config=path_config_file,\n",
        "    trainset_index=0,\n",
        "    shuffle=1,\n",
        ")\n",
        "\n",
        "# Get the pytorch config path\n",
        "pytorch_config_path = loader.model_folder / \"pytorch_config.yaml\"\n",
        "\n",
        "model_cfg = read_config_as_dict(pytorch_config_path)\n",
        "model_cfg['data'][\"train\"][\"affine\"][\"rotation\"]=180\n",
        "\n",
        "# Save the modified config\n",
        "dlc_torch.config.write_config(pytorch_config_path,model_cfg)"
      ],
      "metadata": {
        "id": "8MhtCYmIhIZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 1: Why is label quality crucial for Computer Vision model training?"
      ],
      "metadata": {
        "id": "6BbCe5uruPHx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DLC repo provides images with the labes, review them before starting training"
      ],
      "metadata": {
        "id": "JU7tqpXvvTq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @ title Interactive Labeled Frames Review\n",
        "\n",
        "# Path to CSV and image folder (from cloned repo)\n",
        "csv_path = '/content/cloned-DLC-repo/examples/openfield-Pranav-2018-10-30/labeled-data/m4s1/CollectedData_Pranav.csv'\n",
        "img_folder = '/content/cloned-DLC-repo/examples/openfield-Pranav-2018-10-30/labeled-data/m4s1/'\n",
        "\n",
        "# Parse CSV (skip header rows, focus on data)\n",
        "try:\n",
        "    df = pd.read_csv(csv_path, skiprows=2)\n",
        "    df.columns = ['img_path', 'snout_x', 'snout_y', 'leftear_x', 'leftear_y', 'rightear_x', 'rightear_y', 'tailbase_x', 'tailbase_y']\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: CSV file not found. Ensure the demo data is loaded correctly.\")\n",
        "    raise\n",
        "\n",
        "# Define plotting function for use with interact\n",
        "def plot_labeled_image(idx):\n",
        "    try:\n",
        "        row = df.iloc[idx]\n",
        "        img_name = row['img_path'].split('/')[-1]  # e.g., 'img0000.png'\n",
        "        img_path = os.path.join(img_folder, img_name)\n",
        "\n",
        "        # Load and plot image\n",
        "        img = mpimg.imread(img_path)\n",
        "        fig, ax = plt.subplots(figsize=(8, 6))\n",
        "        ax.imshow(img)\n",
        "\n",
        "        # Overlay body parts as colored points\n",
        "        ax.scatter(row['snout_x'], row['snout_y'], c='r', s=50, label='Snout')\n",
        "        ax.scatter(row['leftear_x'], row['leftear_y'], c='g', s=50, label='Left Ear')\n",
        "        ax.scatter(row['rightear_x'], row['rightear_y'], c='b', s=50, label='Right Ear')\n",
        "        ax.scatter(row['tailbase_x'], row['tailbase_y'], c='y', s=50, label='Tail Base')\n",
        "\n",
        "        ax.legend()\n",
        "        ax.set_title(f\"Labeled Image: {img_name}\")\n",
        "        plt.show()\n",
        "\n",
        "        print(\"Review: Check if points align with body parts. Mislabels? Consider refining in full DLC GUI.\")\n",
        "    except IndexError:\n",
        "        print(f\"Error: Index {idx} out of range. Choose a number between 0 and 114.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Image {img_name} not found. Ensure the demo data is loaded correctly.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error: {e}. Try another index.\")\n",
        "\n",
        "# Create interactive slider\n",
        "print(\"Interactive Label Review: Use the slider to select image indices (0-114) to review labeled data.\")\n",
        "interact(\n",
        "    plot_labeled_image,\n",
        "    idx=IntSlider(min=0, max=len(df)-1, step=1, value=0, description='Image Index')\n",
        ")"
      ],
      "metadata": {
        "id": "itd6m44SwzIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "reply here:"
      ],
      "metadata": {
        "id": "dHp_a3KvvpuQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bonus Problem: Illustrating the Impact of Label Quality on DLC Model Performance\n",
        "\n",
        "Description:\n",
        "\n",
        "To demonstrate why label quality is crucial in computer vision, modify the training labels in CollectedData_Pranav.csv to simulate corruption or shuffling.\n",
        "\n",
        "Train the DLC model on the modified data and compare performance (e.g., mean likelihoods, accuracy) to the original. This illustrates that models can \"learn\" (memorize) even random labels with high train accuracy but fail to generalize, dropping test performance to near-zero"
      ],
      "metadata": {
        "id": "q_1gsRWbTmod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load original CSV\n",
        "gt_path = '/content/cloned-DLC-repo/examples/openfield-Pranav-2018-10-30/labeled-data/m4s1/CollectedData_Pranav.csv'\n",
        "df = pd.read_csv(gt_path, skiprows=2)\n",
        "df.columns = ['img_path', 'snout_x', 'snout_y', 'leftear_x', 'leftear_y', 'rightear_x', 'rightear_y', 'tailbase_x', 'tailbase_y']\n",
        "\n",
        "# Shuffle version\n",
        "df_shuffled = df.copy()\n",
        "df_shuffled.iloc[:,1:] = np.random.permutation(df_shuffled.iloc[:,1:].values)\n",
        "df_shuffled.to_csv('/content/cloned-DLC-repo/examples/openfield-Pranav-2018-10-30/labeled-data/m4s1/CollectedData_Pranav_shuffled.csv', index=False)\n",
        "\n",
        "# Corrupted version\n",
        "df_corrupted = df.copy()\n",
        "df_corrupted.iloc[:,1:] += np.abs(np.random.normal(0, 5, df_corrupted.iloc[:,1:].shape))\n",
        "df_corrupted.to_csv('/content/cloned-DLC-repo/examples/openfield-Pranav-2018-10-30/labeled-data/m4s1/CollectedData_Pranav_corrupted.csv', index=False)\n",
        "\n",
        "# Then, replace CSV in DLC folder and run training/analyze_videos"
      ],
      "metadata": {
        "id": "MUr391ScWSHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start training:\n",
        "This function trains the network for a specific shuffle of the training dataset."
      ],
      "metadata": {
        "id": "KHoImX9F0PqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's also change the display and save_epochs just in case Colab takes away\n",
        "# the GPU... If that happens, you can reload from a saved point using the\n",
        "# `snapshot_path` argument to `deeplabcut.train_network`:\n",
        "#   deeplabcut.train_network(..., snapshot_path=\"/content/.../snapshot-050.pt\")\n",
        "\n",
        "# Typically, you want to train to ~200 epochs. We set the batch size to 8 to\n",
        "# utilize the GPU's capabilities.\n",
        "\n",
        "# More info and there are more things you can set:\n",
        "#   https://deeplabcut.github.io/DeepLabCut/docs/standardDeepLabCut_UserGuide.html#g-train-the-network\n",
        "\n",
        "deeplabcut.train_network(\n",
        "    path_config_file,\n",
        "    shuffle=1,\n",
        "    save_epochs=5,\n",
        "    epochs=20,\n",
        "    batch_size=8,\n",
        ")\n",
        "\n",
        "# This will run until you stop it (CTRL+C), or hit \"STOP\" icon, or when it hits the end."
      ],
      "metadata": {
        "id": "0QLVFW2IieoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @ title Inspect the network training\n",
        "# Adjust path to learning_stats.csv (from DLC model folder)\n",
        "model_folder = '/content/cloned-DLC-repo/examples/openfield-Pranav-2018-10-30/dlc-models-pytorch/iteration-0/openfieldOct30-trainset95shuffle1/train/'\n",
        "csv_path = os.path.join(model_folder, 'learning_stats.csv')\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(csv_path)\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: learning_stats.csv not found. Ensure training completed successfully.\")\n",
        "    raise\n",
        "\n",
        "# Figure with subplots for losses\n",
        "fig, axs = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n",
        "\n",
        "# Plot training losses\n",
        "axs[0].plot(df['step'], df['losses/train.bodypart_heatmap'], label='Train Heatmap Loss', marker='o')\n",
        "axs[0].plot(df['step'], df['losses/train.bodypart_locref'], label='Train Locref Loss', marker='o')\n",
        "axs[0].plot(df['step'], df['losses/train.bodypart_total_loss'], label='Train Bodypart Total Loss', marker='o')\n",
        "axs[0].plot(df['step'], df['losses/train.total_loss'], label='Train Total Loss', marker='o', linewidth=2)\n",
        "axs[0].set_ylabel('Loss')\n",
        "axs[0].set_title('Training Losses over Epochs')\n",
        "axs[0].legend()\n",
        "axs[0].grid(True)\n",
        "\n",
        "# Plot evaluation losses (only where available)\n",
        "eval_df = df.dropna(subset=['losses/eval.total_loss'])\n",
        "axs[1].plot(eval_df['step'], eval_df['losses/eval.bodypart_heatmap'], label='Eval Heatmap Loss', marker='s')\n",
        "axs[1].plot(eval_df['step'], eval_df['losses/eval.bodypart_locref'], label='Eval Locref Loss', marker='s')\n",
        "axs[1].plot(eval_df['step'], eval_df['losses/eval.bodypart_total_loss'], label='Eval Bodypart Total Loss', marker='s')\n",
        "axs[1].plot(eval_df['step'], eval_df['losses/eval.total_loss'], label='Eval Total Loss', marker='s', linewidth=2)\n",
        "axs[1].set_xlabel('Epoch')\n",
        "axs[1].set_ylabel('Loss')\n",
        "axs[1].set_title('Evaluation Losses at Checkpoints')\n",
        "axs[1].legend()\n",
        "axs[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Separate plot for metrics\n",
        "metric_fig, metric_ax = plt.subplots(figsize=(12, 6))\n",
        "eval_metrics = df.dropna(subset=['metrics/test.mAP'])\n",
        "metric_ax.plot(eval_metrics['step'], eval_metrics['metrics/test.mAP'], label='Test mAP (%)', marker='o')\n",
        "metric_ax.plot(eval_metrics['step'], eval_metrics['metrics/test.mAR'], label='Test mAR (%)', marker='o')\n",
        "# Twin axis for RMSE (different scale)\n",
        "rmse_ax = metric_ax.twinx()\n",
        "rmse_ax.plot(eval_metrics['step'], eval_metrics['metrics/test.rmse'], label='Test RMSE (pixels)', marker='o', color='r')\n",
        "rmse_ax.plot(eval_metrics['step'], eval_metrics['metrics/test.rmse_pcutoff'], label='Test RMSE (pcutoff, pixels)', marker='o', color='m')\n",
        "rmse_ax.set_ylabel('RMSE Value')\n",
        "metric_ax.set_xlabel('Epoch')\n",
        "metric_ax.set_ylabel('mAP/mAR Value')\n",
        "metric_ax.set_title('Evaluation Metrics over Epochs')\n",
        "metric_ax.legend(loc='upper left')\n",
        "rmse_ax.legend(loc='upper right')\n",
        "metric_ax.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"Review: Analyze if train/eval losses are decreasing without divergence (overfitting if eval rises while train drops). Metrics like mAP/mAR should increase, RMSE decrease. If metrics plateau, consider more epochs or hyperparameter tuning.\")"
      ],
      "metadata": {
        "id": "vZZu-UhS0EJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start evaluating:\n",
        "This function evaluates a trained model for a specific shuffle/shuffles at a particular state or all the states on the data set (images)\n",
        "and stores the results as .csv file in a subdirectory under **evaluation-results**"
      ],
      "metadata": {
        "id": "z_IrgzSP1NBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deeplabcut.evaluate_network(path_config_file, plotting=True)\n",
        "\n",
        "# Here you want to see a low pixel error! Of course, it can only be as\n",
        "# good as the labeler, so be sure your labels are good!a"
      ],
      "metadata": {
        "id": "-rPRVlrQiklr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 2: What metrics indicate good training?"
      ],
      "metadata": {
        "id": "l4ozdTYst9I7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "reply here:"
      ],
      "metadata": {
        "id": "trg4NlP83I0x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start Analyzing videos:\n",
        "This function analyzes the new video. The user can choose the best model from the evaluation results and specify the correct snapshot index for the variable **snapshotindex** in the **config.yaml** file. Otherwise, by default the most recent snapshot is used to analyse the video.\n",
        "\n",
        "The results are stored in hd5 file in the same directory where the video resides.\n",
        "\n",
        "**On the demo data, this should take around ~ 90 seconds! (The demo frames are 640x480, which should run around 25 FPS on the google-provided T4 GPU)**"
      ],
      "metadata": {
        "id": "oago4RCB3Ht1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter the list of videos to analyze.\n",
        "videofile_path = [\"/content/cloned-DLC-repo/examples/openfield-Pranav-2018-10-30/videos/m3v1mp4.mp4\"]\n",
        "deeplabcut.analyze_videos(path_config_file, videofile_path, videotype=\".mp4\")"
      ],
      "metadata": {
        "id": "HfHqNa5KjUS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deeplabcut.create_labeled_video(path_config_file, videofile_path)"
      ],
      "metadata": {
        "id": "_kxpzY8Hjelz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deeplabcut.create_labeled_video(\n",
        "    path_config_file,\n",
        "    videofile_path,\n",
        "    videotype='mp4',\n",
        "    filtered=False\n",
        ")"
      ],
      "metadata": {
        "id": "KabbpIgtHri9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deeplabcut.plot_trajectories(path_config_file, videofile_path)"
      ],
      "metadata": {
        "id": "W637_qjEjjx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 3: Evaluate predictions"
      ],
      "metadata": {
        "id": "GgySH0AEubjW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Inspect the likelihood-over-time plot (plot-likelihood.png in the plot-poses folder).\n",
        "\n",
        "2. Adapt the code to load the HDF5 output file generated by deeplabcut.analyze_videos.\n",
        "\n",
        "3. Extract likelihood values for each body part, and visualize their distributions using histograms and kernel density estimates (KDEs).\n",
        "\n",
        "Address the following questions:\n",
        "Which body part has the lowest mean likelihood?\n",
        "What percentage of frames show low likelihoods ?\n",
        "\n",
        "Based on both the distributions and the likelihood-over-time plot, propose at least two targeted improvements to the DLC model (for example, adding specific viewpoints to the training set, refining labels for difficult frames, or increasing model iterations)."
      ],
      "metadata": {
        "id": "jGoEuiZVNkEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust path to predictions HDF5 file, see output of deeplabcut.analyze_videos line with \"Saving results in \"\n",
        "h5_path = '/content/cloned-DLC-repo/examples/openfield-Pranav-2018-10-30/videos/m3v1mp4DLC_Resnet50_openfieldOct30shuffle1_snapshot_best-20.h5'\n",
        "\n",
        "# Load predictions from HDF5\n",
        "try:\n",
        "    data = pd.read_hdf(h5_path)\n",
        "    scorer = data.columns.levels[0][0]\n",
        "    bodyparts = ['snout', 'leftear', 'rightear', 'tailbase']\n",
        "    likelihood_columns = [(scorer, bp, 'likelihood') for bp in bodyparts]\n",
        "    if not all(col in data.columns for col in likelihood_columns):\n",
        "        print(f\"Warning: Expected likelihood columns {likelihood_columns} not found. Available columns: {list(data.columns)}\")\n",
        "        raise ValueError(\"Missing expected columns in HDF5 file.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: HDF5 file not found at {h5_path}. Ensure deeplabcut.analyze_videos ran successfully.\")\n",
        "    raise\n",
        "except Exception as e:\n",
        "    print(f\"Unexpected error loading HDF5: {e}. Inspect with: h5py.File('{h5_path}', 'r').keys()\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "JxqG5IJbzk96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# solution"
      ],
      "metadata": {
        "id": "p4vU3kkyRlX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cdB0DNURYyAX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bonus: DCL Network Architecture"
      ],
      "metadata": {
        "id": "8vl3NT1H2Bmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Graphical visualization with torchviz\n",
        "print(\"\\nGenerating graphical visualization of the model architecture...\")\n",
        "try:\n",
        "    # Install dependencies\n",
        "    %pip install --quiet torchviz graphviz\n",
        "    # Install Graphviz binary for Colab (only if needed)\n",
        "    !if ! command -v dot >/dev/null 2>&1; then apt-get update -qq && apt-get install -y graphviz; fi\n",
        "    from torchviz import make_dot\n",
        "    import os\n",
        "    from IPython.display import Image, display\n",
        "    import torch\n",
        "    import deeplabcut.pose_estimation_pytorch as dlc_torch\n",
        "    from deeplabcut.core.config import read_config_as_dict\n",
        "\n",
        "    # Load model configuration\n",
        "    pytorch_config_path = loader.model_folder / \"pytorch_config.yaml\"\n",
        "    model_cfg = read_config_as_dict(pytorch_config_path)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Initialize the model using DLC's training loader\n",
        "    try:\n",
        "        # Attempt to load the model using DLC's internal utilities\n",
        "        from deeplabcut.pose_estimation_pytorch.training import PoseTrainingRunner\n",
        "        runner = PoseTrainingRunner(\n",
        "            config=pytorch_config_path,\n",
        "            trainset_index=0,\n",
        "            shuffle=1,\n",
        "        )\n",
        "        model = runner.model.to(device)\n",
        "    except (AttributeError, ImportError) as e:\n",
        "        print(f\"Error initializing model: {e}\")\n",
        "        print(\"Model loading failed. Falling back to configuration-based description.\")\n",
        "        model = None\n",
        "\n",
        "    if model:\n",
        "        # Create dummy input for forward pass\n",
        "        x = torch.randn(1, 3, 448, 448).to(device)  # Input size: batch=1, channels=3, 448x448 (from train.txt)\n",
        "\n",
        "        # Perform forward pass\n",
        "        y = model(x)\n",
        "\n",
        "        # Generate and save visualization\n",
        "        output_dir = loader.model_folder\n",
        "        os.makedirs(output_dir, exist_ok=True)  # Ensure output directory exists\n",
        "        output_path = os.path.join(output_dir, \"model_architecture\")\n",
        "        dot = make_dot(y, params=dict(model.named_parameters()), show_attrs=True, show_saved=True)\n",
        "        dot.format = 'png'\n",
        "        dot.render(output_path, view=False)  # Save as PNG without opening\n",
        "\n",
        "        # Verify and display output\n",
        "        output_file = f\"{output_path}.png\"\n",
        "        if os.path.exists(output_file):\n",
        "            print(f\"Graphical model architecture saved as {output_file}\")\n",
        "            display(Image(filename=output_file))\n",
        "            print(\"Download the PNG from Colab's file explorer (left sidebar) under the model folder.\")\n",
        "        else:\n",
        "            print(f\"Warning: Output file {output_file} not found. Rendering may have failed.\")\n",
        "            print(\"Fallback: Displaying graph as text representation...\")\n",
        "            print(dot)\n",
        "    else:\n",
        "        # Fallback: Describe architecture from configuration\n",
        "        print(\"\\nFallback: Model visualization failed. Describing architecture from configuration:\")\n",
        "        print(f\"Network Type: {model_cfg.get('net_type', 'N/A')}\")\n",
        "        print(f\"Backbone: {model_cfg['model']['backbone']['type']} ({model_cfg['model']['backbone']['model_name']})\")\n",
        "        print(f\"Bodyparts: {model_cfg['metadata']['bodyparts']}\")\n",
        "        print(f\"Number of Keypoints: {len(model_cfg['metadata']['bodyparts'])}\")\n",
        "        print(f\"Input Size: 448x448 (from crop_sampling)\")\n",
        "        print(f\"Output: Heatmaps ({model_cfg['model']['heads']['bodypart']['heatmap_config']['channels'][-1]} channels) and location refinements for {len(model_cfg['metadata']['bodyparts'])} keypoints\")\n",
        "        print(f\"Location Refinement Std: {model_cfg['model']['heads']['bodypart']['locref_std']}\")\n",
        "        print(\"Note: The model uses a ResNet50 backbone for feature extraction, followed by a heatmap head (predicting keypoint probabilities) and a location refinement head (refining coordinates).\")\n",
        "\n",
        "    print(\"Review: If visualized, inspect the network graph to identify the ResNet50 backbone, heatmap head, and location refinement head. How do ResNet50‚Äôs convolutional layers capture spatial features for mouse tracking? Why are separate heads used for heatmaps and location refinement in pose estimation for bodyparts (snout, leftear, rightear, tailbase)? If visualization failed, use the configuration details to infer the model structure. How does the 448x448 input size balance detail and computational cost? Consider checking the DLC version or re-running '%pip install deeplabcut --upgrade'.\")\n",
        "\n",
        "except ImportError as e:\n",
        "    print(f\"Error: Failed to import torchviz, graphviz, or DLC modules: {e}\")\n",
        "    print(\"Run '%pip install torchviz graphviz deeplabcut --upgrade' in a new cell.\")\n",
        "except NameError as e:\n",
        "    print(f\"Error: Variable not defined: {e}\")\n",
        "    print(\"Ensure 'loader' is defined. Check if deeplabcut.create_training_dataset was run successfully.\")\n",
        "except RuntimeError as e:\n",
        "    print(f\"Error during forward pass: {e}\")\n",
        "    print(\"Verify model initialization, CUDA availability, and input size (448x448). Try re-running the model setup.\")\n",
        "except OSError as e:\n",
        "    print(f\"Error with Graphviz binary: {e}\")\n",
        "    print(\"Run '!apt-get install graphviz' in a new cell. Verify 'dot' is in PATH with '!which dot'.\")\n",
        "except Exception as e:\n",
        "    print(f\"Unexpected error: {e}\")\n",
        "    print(\"Try re-running the cell or installing dependencies manually ('%pip install torchviz graphviz deeplabcut --upgrade' and '!apt-get install graphviz').\")"
      ],
      "metadata": {
        "id": "W6n0XqLC1-HH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}